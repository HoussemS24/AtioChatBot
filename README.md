# ATIO Chatbot - RAG-basierter Chatbot mit Ollama

Ein vollstÃ¤ndig funktionsfÃ¤higer RAG (Retrieval-Augmented Generation) Chatbot fÃ¼r die ATIO-Website, der lokal mit Ollama lÃ¤uft und keine Docker-Installation benÃ¶tigt.

## ğŸ¯ Features

- **RAG-System**: Intelligente Dokumentensuche basierend auf der ATIO Knowledge Base
- **Ollama Integration**: Nutzt lokale LLM-Modelle (Mistral, Llama 2, etc.)
- **FastAPI Backend**: Moderner REST API Server
- **Responsive Frontend**: SchÃ¶nes Chatbot-Widget fÃ¼r Websites
- **Windows-kompatibel**: Einfache Installation ohne Docker
- **Mehrsprachig**: UnterstÃ¼tzt Deutsch und Englisch

## ğŸ“‹ Voraussetzungen

### Erforderlich
- **Python 3.8+** - [Download](https://www.python.org/)
- **Ollama** - [Download](https://ollama.ai)
- **Windows 10/11** oder Linux/macOS

### Optional
- Git fÃ¼r Versionskontrolle
- Visual Studio Code fÃ¼r Entwicklung

## ğŸš€ Installation

### Schritt 1: Ollama installieren

1. Laden Sie Ollama von [https://ollama.ai](https://ollama.ai) herunter
2. FÃ¼hren Sie das Installationsprogramm aus
3. Nach der Installation kÃ¶nnen Sie Ollama starten

### Schritt 2: Projekt klonen oder herunterladen

```bash
# Mit Git
git clone https://github.com/HoussemS24/AtioChatBot.git
cd AtioChatBot

# Oder manuell herunterladen und entpacken
```

### Schritt 3: Setup ausfÃ¼hren

#### Option A: Batch-Skript (einfacher)
```bash
setup.bat
```

#### Option B: PowerShell-Skript
```powershell
powershell -ExecutionPolicy Bypass -File setup.ps1
```

#### Option C: Manuell
```bash
pip install -r requirements.txt
mkdir data
mkdir logs
```

## ğŸ“– Verwendung

### 1. Ollama starten

Ã–ffnen Sie eine neue Eingabeaufforderung und geben Sie ein:

```bash
ollama serve
```

Dies startet den Ollama-Server auf `http://localhost:11434`

### 2. Chatbot Backend starten

Ã–ffnen Sie eine zweite Eingabeaufforderung im Projektverzeichnis:

```bash
python app.py
```

Der Server lÃ¤uft nun auf `http://localhost:8000`

### 3. Chatbot Ã¶ffnen

#### Option A: HTML-Datei direkt Ã¶ffnen
```bash
# Doppelklick auf: static/index.html
```

#### Option B: Im Browser Ã¶ffnen
```
http://localhost:8000/static/index.html
```

#### Option C: API-Dokumentation
```
http://localhost:8000/docs
```

### 4. Mit dem Chatbot interagieren

- Klicken Sie auf den Chatbot-Button in der unteren rechten Ecke
- Stellen Sie Fragen Ã¼ber ATIO und seine Dienstleistungen
- Der Chatbot nutzt die Knowledge Base, um relevante Antworten zu geben

## ğŸ—ï¸ Projektstruktur

```
AtioChatBot/
â”œâ”€â”€ app.py                          # FastAPI Backend
â”œâ”€â”€ rag_system.py                   # RAG-System mit Knowledge Base
â”œâ”€â”€ requirements.txt                # Python AbhÃ¤ngigkeiten
â”œâ”€â”€ setup.bat                       # Windows Setup (Batch)
â”œâ”€â”€ setup.ps1                       # Windows Setup (PowerShell)
â”œâ”€â”€ README.md                       # Diese Datei
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ atio_knowledge_base.json   # ATIO Knowledge Base
â”‚   â””â”€â”€ atio_rag.db                # SQLite RAG Datenbank (wird erstellt)
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html                 # Chatbot Frontend
â”‚   â”œâ”€â”€ style.css                  # Chatbot Styling
â”‚   â””â”€â”€ script.js                  # Chatbot JavaScript
â””â”€â”€ logs/                          # Log-Dateien
```

## ğŸ”§ Konfiguration

### Umgebungsvariablen

Sie kÃ¶nnen diese Variablen setzen, um das Verhalten anzupassen:

```bash
# Windows CMD
set OLLAMA_BASE_URL=http://localhost:11434
set OLLAMA_MODEL=mistral
set RAG_DB_PATH=data/atio_rag.db

# Windows PowerShell
$env:OLLAMA_BASE_URL="http://localhost:11434"
$env:OLLAMA_MODEL="mistral"
$env:RAG_DB_PATH="data/atio_rag.db"

# Linux/macOS
export OLLAMA_BASE_URL=http://localhost:11434
export OLLAMA_MODEL=mistral
export RAG_DB_PATH=data/atio_rag.db
```

### VerfÃ¼gbare Ollama Modelle

- **mistral** (7B) - Schnell und zuverlÃ¤ssig (empfohlen)
- **llama2** (7B) - Gutes VerstÃ¤ndnis
- **neural-chat** (7B) - Spezialisiert auf Chat
- **orca-mini** (3B) - Sehr schnell, weniger Speicher

Modell herunterladen:
```bash
ollama pull mistral
ollama pull llama2
```

## ğŸ¨ Anpassung

### Knowledge Base bearbeiten

Bearbeiten Sie `data/atio_knowledge_base.json`, um Inhalte hinzuzufÃ¼gen oder zu Ã¤ndern:

```json
{
  "solutions": [
    {
      "name": "Neue LÃ¶sung",
      "description": "Beschreibung...",
      "features": [...]
    }
  ]
}
```

Nach Ã„nderungen neu starten Sie den Backend-Server.

### Chatbot-Styling anpassen

Bearbeiten Sie `static/style.css`:

```css
/* Farben Ã¤ndern */
.chatbot-header {
    background: linear-gradient(135deg, #003366 0%, #004d99 100%);
}
```

### Frontend-Text anpassen

Bearbeiten Sie `static/index.html`:

```html
<h2>ATIO Chatbot</h2>
<p>Fragen Sie mich Ã¼ber ATIO und unsere IoT-LÃ¶sungen</p>
```

## ğŸ› Fehlerbehebung

### Problem: "Ollama ist nicht erreichbar"

**LÃ¶sung:**
1. Stellen Sie sicher, dass Ollama lÃ¤uft (`ollama serve`)
2. ÃœberprÃ¼fen Sie die URL: `http://localhost:11434`
3. Firewall-Einstellungen Ã¼berprÃ¼fen

### Problem: "ModuleNotFoundError: No module named 'fastapi'"

**LÃ¶sung:**
```bash
pip install -r requirements.txt
```

### Problem: "Port 8000 ist bereits in Verwendung"

**LÃ¶sung:**
```bash
# Ã„ndern Sie den Port in app.py
# Zeile: uvicorn.run(app, host="0.0.0.0", port=8001)
```

### Problem: Chatbot antwortet nicht

**ÃœberprÃ¼fung:**
1. Ollama lÃ¤uft? (`ollama serve`)
2. Backend lÃ¤uft? (`python app.py`)
3. Modell heruntergeladen? (`ollama pull mistral`)
4. Browser-Konsole auf Fehler Ã¼berprÃ¼fen (F12)

## ğŸ“Š API Endpoints

### Health Check
```
GET /health
```

Antwortet mit:
```json
{
  "status": "healthy",
  "ollama_connected": true,
  "rag_ready": true,
  "model": "mistral"
}
```

### Chat
```
POST /chat
Content-Type: application/json

{
  "message": "Was ist atio?",
  "conversation_id": "conv_123",
  "language": "de"
}
```

### Knowledge Base Suche
```
GET /rag/search?query=IoT&limit=5
```

### Info
```
GET /info
```

## ğŸ”’ Sicherheit

- Der Chatbot lÃ¤uft lokal auf Ihrem Computer
- Keine Daten werden an externe Server gesendet
- Ollama lÃ¤uft ebenfalls lokal
- Alle Kommunikation ist lokal

## ğŸ“ˆ Performance

- **Erste Antwort**: 2-5 Sekunden (abhÃ¤ngig vom Modell)
- **Speicherverbrauch**: 2-8 GB (abhÃ¤ngig vom Modell)
- **CPU-Auslastung**: Moderat wÃ¤hrend der Verarbeitung

### Optimierungstipps

1. Verwenden Sie ein kleineres Modell (z.B. orca-mini)
2. ErhÃ¶hen Sie den RAM auf mindestens 8 GB
3. Verwenden Sie eine SSD fÃ¼r bessere Performance
4. SchlieÃŸen Sie andere Anwendungen

## ğŸš€ Deployment

### Lokal testen
```bash
python app.py
```

### Auf Website integrieren

FÃ¼gen Sie diesen Code in Ihre Website ein:

```html
<!-- ATIO Chatbot Widget -->
<script src="http://localhost:8000/static/script.js"></script>
<link rel="stylesheet" href="http://localhost:8000/static/style.css">
<div id="chatbot-container"></div>
```

### Production Deployment

FÃ¼r Production-Umgebungen:

1. Verwenden Sie einen WSGI-Server (Gunicorn)
2. Setzen Sie einen Reverse Proxy (Nginx)
3. Verwenden Sie HTTPS
4. Implementieren Sie Rate Limiting
5. FÃ¼gen Sie Authentifizierung hinzu

## ğŸ“ Lizenz

Dieses Projekt ist fÃ¼r die Verwendung mit ATIO konzipiert.

## ğŸ¤ Support

Bei Fragen oder Problemen:

1. ÃœberprÃ¼fen Sie die Fehlerbehebung oben
2. Schauen Sie in die Logs
3. Kontaktieren Sie ATIO: info@atio.de

## ğŸ“š Weitere Ressourcen

- [Ollama Dokumentation](https://ollama.ai)
- [FastAPI Dokumentation](https://fastapi.tiangolo.com/)
- [ATIO Website](https://www.atio.de/)

## âœ¨ Changelog

### v1.0.0 (2026-01-13)
- Initial Release
- RAG-System mit SQLite
- FastAPI Backend
- Responsive Frontend
- Windows Setup-Skripte
- VollstÃ¤ndige Dokumentation

---

**Viel SpaÃŸ mit dem ATIO Chatbot! ğŸ‰**
